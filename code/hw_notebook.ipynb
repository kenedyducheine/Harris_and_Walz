{
 "cells": [
  {
   "cell_type": "code",
   "id": "98fa81fdc80b45cc",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-23T00:59:05.941016Z",
     "start_time": "2024-08-23T00:59:05.886817Z"
    }
   },
   "source": [
    "# Harris and Walz Notebook\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T00:59:05.958985Z",
     "start_time": "2024-08-23T00:59:05.945201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('params.json') as param_json:\n",
    "    params = json.load(param_json)\n",
    "\n",
    "CLIENT_ID = params['CLIENT_ID']\n",
    "SECRET_TOKEN = params['SECRET_TOKEN']\n",
    "USERNAME = params['USERNAME']\n",
    "PASSWORD = params['PASSWORD']\n",
    "KEYWORDS = params['KEYWORDS']\n",
    "keywords = [word.lower() for word in KEYWORDS]"
   ],
   "id": "ff500ea53e7f092a",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T00:59:07.255392Z",
     "start_time": "2024-08-23T00:59:05.960254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_TOKEN)\n",
    "    \n",
    "    # here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': USERNAME,\n",
    "         'password': PASSWORD}\n",
    "    \n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "    \n",
    "    # send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                     auth=auth, data=data, headers=headers)\n",
    "    \n",
    "    # convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "    \n",
    "    # add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "    \n",
    "    # while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n"
   ],
   "id": "372a46c176fe460c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T00:59:07.266860Z",
     "start_time": "2024-08-23T00:59:07.257266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_post(subreddit, cats, concated_df):\n",
    "    '''\n",
    "    Gets post from passed subreddits. \n",
    "    params: \n",
    "        subreddit - the subreddit to pull post from \n",
    "        cats - 'hot' or 'rising' which are filters on subreddits to find trending post \n",
    "        concated_df - dataframe from yesterday which contains post from previous day. \n",
    "    '''\n",
    "\n",
    "    res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}/{cats}\",\n",
    "                       headers=headers)\n",
    "    data = res.json()\n",
    "    #print(data)\n",
    "    posts = data['data']['children']\n",
    "    posts_data = []\n",
    "\n",
    "    for post in posts:\n",
    "        post_info = post['data']\n",
    "        posts_data.append({\n",
    "            'title': post_info['title'],\n",
    "            'upvote_ratio': post_info['upvote_ratio'],\n",
    "            'subreddit_name_prefixed': post_info['subreddit_name_prefixed'],\n",
    "            'date': post_info['created_utc']\n",
    "\n",
    "        })\n",
    "\n",
    "        # Create a DataFrame\n",
    "    df = pd.DataFrame(posts_data)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], unit='s').dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "    df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "    df = df[df['title'].apply(lambda word: any(keyword in word for keyword in keywords))]\n",
    "\n",
    "    newDF = pd.concat([df, concated_df], ignore_index=True)\n",
    "\n",
    "    return newDF\n"
   ],
   "id": "496f1b37ea5e784a",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T00:59:07.276299Z",
     "start_time": "2024-08-23T00:59:07.269775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subreddits():\n",
    "    '''\n",
    "    Runs the \"get_post\" function on all of our subreddits. Outputs a new csv called 'titles'. \n",
    "    Titles contains: \n",
    "        - post title \n",
    "        - subreddit name\n",
    "        - ratio of upvotes to downvotes\n",
    "        - date published \n",
    "    :return: \n",
    "    '''\n",
    "    \n",
    "    with open('output/titles.csv') as maindf:\n",
    "        main_df = pd.read_csv(maindf)\n",
    "\n",
    "    df = get_post(\"politics\", \"hot\", main_df)\n",
    "    df = get_post(\"democrats\", \"rising\", df)\n",
    "    df = get_post(\"politicaldiscussion\", \"rising\", df)\n",
    "    df = get_post(\"politicaldiscussion\", \"hot\", df)\n",
    "    df = get_post(\"moderatepolitics\", \"rising\", df)\n",
    "    df = get_post(\"moderatepolitics\", \"hot\", df)\n",
    "    df = get_post(\"democrats\", \"hot\", df)\n",
    "    df = get_post(\"politics\", \"rising\", df)\n",
    "    df = get_post(\"politics\", \"hot\", df)\n",
    "    df = get_post(\"republicans\", \"rising\", df)\n",
    "    df = get_post(\"republicans\", \"hot\", df)\n",
    "    df = df[['title', 'upvote_ratio', 'subreddit_name_prefixed', 'date']]\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv('output/titles.csv')\n"
   ],
   "id": "6844ceca350994ef",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T00:59:14.303155Z",
     "start_time": "2024-08-23T00:59:07.277555Z"
    }
   },
   "cell_type": "code",
   "source": "subreddits()",
   "id": "b1e175282e7e7af8",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T00:59:14.319542Z",
     "start_time": "2024-08-23T00:59:14.304393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('output/titles.csv')\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "df.to_csv('output/titles.csv')"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n",
      "687\n"
     ]
    }
   ],
   "execution_count": 110
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
