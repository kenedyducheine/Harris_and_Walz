{
 "cells": [
  {
   "cell_type": "code",
   "id": "98fa81fdc80b45cc",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T21:51:54.201157Z",
     "start_time": "2024-08-27T21:51:54.072478Z"
    }
   },
   "source": [
    "# Harris and Walz Notebook\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud"
   ],
   "outputs": [],
   "execution_count": 269
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:51:54.222638Z",
     "start_time": "2024-08-27T21:51:54.207091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('params.json') as param_json:\n",
    "    params = json.load(param_json)\n",
    "\n",
    "CLIENT_ID = params['CLIENT_ID']\n",
    "SECRET_TOKEN = params['SECRET_TOKEN']\n",
    "USERNAME = params['USERNAME']\n",
    "PASSWORD = params['PASSWORD']\n",
    "KEYWORDS = params['KEYWORDS']\n",
    "keywords = [word.lower() for word in KEYWORDS]"
   ],
   "id": "ff500ea53e7f092a",
   "outputs": [],
   "execution_count": 270
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:51:55.565478Z",
     "start_time": "2024-08-27T21:51:54.224764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_TOKEN)\n",
    "    \n",
    "    # here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': USERNAME,\n",
    "         'password': PASSWORD}\n",
    "    \n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "    \n",
    "    # send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                     auth=auth, data=data, headers=headers)\n",
    "    \n",
    "    # convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "    \n",
    "    # add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "    \n",
    "    # while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n"
   ],
   "id": "372a46c176fe460c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 271
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:51:55.580111Z",
     "start_time": "2024-08-27T21:51:55.568345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_post(subreddit, cats, concated_df):\n",
    "    '''\n",
    "    Gets post from passed subreddits. \n",
    "    params: \n",
    "        subreddit - the subreddit to pull post from \n",
    "        cats - 'hot' or 'rising' which are filters on subreddits to find trending post \n",
    "        concated_df - dataframe from yesterday which contains post from previous day. \n",
    "    '''\n",
    "\n",
    "    res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}/{cats}\",\n",
    "                       headers=headers)\n",
    "    data = res.json()\n",
    "    #print(data)\n",
    "    posts = data['data']['children']\n",
    "    posts_data = []\n",
    "\n",
    "    for post in posts:\n",
    "        post_info = post['data']\n",
    "        posts_data.append({\n",
    "            'title': post_info['title'],\n",
    "            'upvote_ratio': post_info['upvote_ratio'],\n",
    "            'subreddit_name_prefixed': post_info['subreddit_name_prefixed'],\n",
    "            'date': post_info['created_utc']\n",
    "\n",
    "        })\n",
    "\n",
    "        # Create a DataFrame\n",
    "    df = pd.DataFrame(posts_data)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], unit='s').dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "    df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "    df = df[df['title'].apply(lambda word: any(keyword in word for keyword in keywords))]\n",
    "\n",
    "    newDF = pd.concat([df, concated_df], ignore_index=True)\n",
    "\n",
    "    return newDF\n"
   ],
   "id": "496f1b37ea5e784a",
   "outputs": [],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:51:55.590545Z",
     "start_time": "2024-08-27T21:51:55.581448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subreddits():\n",
    "    '''\n",
    "    Runs the \"get_post\" function on all of our subreddits. Outputs a new csv called 'titles'. \n",
    "    Titles contains: \n",
    "        - post title \n",
    "        - subreddit name\n",
    "        - ratio of upvotes to downvotes\n",
    "        - date published \n",
    "    :return: \n",
    "    '''\n",
    "    \n",
    "    with open('output/titles.csv') as maindf:\n",
    "        main_df = pd.read_csv(maindf)\n",
    "\n",
    "    df = get_post(\"politics\", \"hot\", main_df)\n",
    "    df = get_post(\"democrats\", \"rising\", df)\n",
    "    df = get_post(\"politicaldiscussion\", \"rising\", df)\n",
    "    df = get_post(\"politicaldiscussion\", \"hot\", df)\n",
    "    df = get_post(\"moderatepolitics\", \"rising\", df)\n",
    "    df = get_post(\"moderatepolitics\", \"hot\", df)\n",
    "    df = get_post(\"democrats\", \"hot\", df)\n",
    "    df = get_post(\"politics\", \"rising\", df)\n",
    "    df = get_post(\"politics\", \"hot\", df)\n",
    "    df = get_post(\"republicans\", \"rising\", df)\n",
    "    df = get_post(\"republicans\", \"hot\", df)\n",
    "    df = df[['title', 'upvote_ratio', 'subreddit_name_prefixed', 'date']]\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv('output/titles.csv')\n"
   ],
   "id": "6844ceca350994ef",
   "outputs": [],
   "execution_count": 273
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:52:07.534236Z",
     "start_time": "2024-08-27T21:51:58.618942Z"
    }
   },
   "cell_type": "code",
   "source": "subreddits()",
   "id": "b1e175282e7e7af8",
   "outputs": [],
   "execution_count": 274
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:53:43.731119Z",
     "start_time": "2024-08-27T21:53:43.671113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('output/titles.csv')\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "df.to_csv('output/titles.csv')"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904\n",
      "900\n"
     ]
    }
   ],
   "execution_count": 276
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Word Clouds! ",
   "id": "2f744cb1213c796c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:53:55.640121Z",
     "start_time": "2024-08-27T21:53:55.402408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "sw = set(stopwords.words('english'))\n",
    "sw.update(['harris', 'kamala', 'kamala harris', 'walz', 'tim walz', 'tim', 'say', 'lol', 'says', 'look', 'gave', 'see', 'new', 'likely', 'harriswalz', 'today', 'calls', 'still'])"
   ],
   "id": "64e04e462f3f0872",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/KenedyDucheine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 277
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:53:57.088135Z",
     "start_time": "2024-08-27T21:53:57.070267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wc(dataframe):\n",
    "    df = dataframe\n",
    "    for i in df['subreddit_name_prefixed']: \n",
    "        \n",
    "        newdf = df[df['subreddit_name_prefixed']== i]\n",
    "        v = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        newdf = newdf.drop_duplicates(subset = ['title'])\n",
    "        newdf['title'] = newdf['title'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "        textlist = [word_tokenize(w) for w in newdf['title']]\n",
    "        tkwords = [word for n in textlist for word in n if word.lower() not in sw]\n",
    "        strings = ' '.join(tkwords)\n",
    "        \n",
    "        wc = WordCloud(background_color= '#FFFFFF', colormap= 'cividis').generate(strings)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.axis('off')\n",
    "        plt.figtext(0.3,0.8,(f'{v} WordCloud: {date.today()}'),fontweight= 'bold')\n",
    "        plt.imshow(wc)\n",
    "        plt.savefig(f'output/wordclouds/{v}_wordcloud_{date.today()}.png')\n",
    "        plt.close()\n",
    "      \n",
    "\n"
   ],
   "id": "a797e1990a3d044d",
   "outputs": [],
   "execution_count": 278
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-27T21:54:01.414621Z"
    }
   },
   "cell_type": "code",
   "source": "wc(df)",
   "id": "e41073431e13e86b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/9gfjf8kd4s9b32tscxh_tlh40000gn/T/ipykernel_18297/3290662681.py:16: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 8))\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
