{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fa81fdc80b45cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:17:27.080443Z",
     "start_time": "2024-09-05T00:17:26.906367Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory before\n",
      "/Users/KenedyDucheine/DataspellProjects/Harris_and_Walz/code\n",
      "\n",
      "Current working directory before\n",
      "/Users/KenedyDucheine/DataspellProjects/Harris_and_Walz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Harris and Walz Notebook\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "\n",
    "from datetime import date\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to Get the current\n",
    "# working directory\n",
    "def current_path():\n",
    "    print(\"Current working directory before\")\n",
    "    print(os.getcwd())\n",
    "    print()\n",
    "\n",
    "\n",
    "# Driver's code\n",
    "# Printing CWD before\n",
    "current_path()\n",
    "os.chdir('/Users/KenedyDucheine/DataspellProjects/Harris_and_Walz')\n",
    "current_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff500ea53e7f092a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:17:27.147652Z",
     "start_time": "2024-09-05T00:17:27.130204Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('params.json') as param_json:\n",
    "    params = json.load(param_json)\n",
    "\n",
    "CLIENT_ID = params['CLIENT_ID']\n",
    "SECRET_TOKEN = params['SECRET_TOKEN']\n",
    "USERNAME = params['USERNAME']\n",
    "PASSWORD = params['PASSWORD']\n",
    "KEYWORDS = params['KEYWORDS']\n",
    "keywords = [word.lower() for word in KEYWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372a46c176fe460c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:17:28.462149Z",
     "start_time": "2024-09-05T00:17:27.200183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_TOKEN)\n",
    "    \n",
    "    # here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': USERNAME,\n",
    "         'password': PASSWORD}\n",
    "    \n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "    \n",
    "    # send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                     auth=auth, data=data, headers=headers)\n",
    "    \n",
    "    # convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "    \n",
    "    # add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "    \n",
    "    # while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496f1b37ea5e784a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:17:28.511806Z",
     "start_time": "2024-09-05T00:17:28.469406Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_post(subreddit, cats, concated_df):\n",
    "    '''\n",
    "    Gets post from passed subreddits. \n",
    "    params: \n",
    "        subreddit - the subreddit to pull post from \n",
    "        cats - 'hot' or 'rising' which are filters on subreddits to find trending post \n",
    "        concated_df - dataframe from yesterday which contains post from previous day. \n",
    "    '''\n",
    "\n",
    "    res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}/{cats}\",\n",
    "                       headers=headers)\n",
    "    data = res.json()\n",
    "    #print(data)\n",
    "    posts = data['data']['children']\n",
    "    posts_data = []\n",
    "\n",
    "    for post in posts:\n",
    "        post_info = post['data']\n",
    "        posts_data.append({\n",
    "            'title': post_info['title'],\n",
    "            'upvote_ratio': post_info['upvote_ratio'],\n",
    "            'subreddit_name_prefixed': post_info['subreddit_name_prefixed'],\n",
    "            'date': post_info['created_utc']\n",
    "\n",
    "        })\n",
    "\n",
    "        # Create a DataFrame\n",
    "    df = pd.DataFrame(posts_data)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], unit='s').dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "    df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "    df = df[df['title'].apply(lambda word: any(keyword in word for keyword in keywords))]\n",
    "\n",
    "    newDF = pd.concat([df, concated_df], ignore_index=True)\n",
    "\n",
    "    return newDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6844ceca350994ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:17:28.607517Z",
     "start_time": "2024-09-05T00:17:28.550523Z"
    }
   },
   "outputs": [],
   "source": [
    "def subreddits():\n",
    "    '''\n",
    "    Runs the \"get_post\" function on all of our subreddits. Outputs a new csv called 'titles'. \n",
    "    Titles contains: \n",
    "        - post title \n",
    "        - subreddit name\n",
    "        - ratio of upvotes to downvotes\n",
    "        - date published \n",
    "    :return: \n",
    "    '''\n",
    "    \n",
    "    with open('output/titles.csv') as maindf:\n",
    "        main_df = pd.read_csv(maindf)\n",
    "\n",
    "    df = get_post(\"politics\", \"hot\", main_df)\n",
    "    df = get_post(\"democrats\", \"rising\", df)\n",
    "    df = get_post(\"politicaldiscussion\", \"rising\", df)\n",
    "    df = get_post(\"politicaldiscussion\", \"hot\", df)\n",
    "    df = get_post(\"moderatepolitics\", \"rising\", df)\n",
    "    df = get_post(\"moderatepolitics\", \"hot\", df)\n",
    "    df = get_post(\"democrats\", \"hot\", df)\n",
    "    df = get_post(\"politics\", \"rising\", df)\n",
    "    df = get_post(\"politics\", \"hot\", df)\n",
    "    df = get_post(\"republicans\", \"rising\", df)\n",
    "    df = get_post(\"republicans\", \"hot\", df)\n",
    "    df = df[['title', 'upvote_ratio', 'subreddit_name_prefixed', 'date']]\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv('output/titles.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e175282e7e7af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T00:17:35.552117Z",
     "start_time": "2024-09-05T00:17:28.619277Z"
    }
   },
   "outputs": [],
   "source": [
    "subreddits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T13:13:57.466553Z",
     "start_time": "2024-09-03T13:13:57.445659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1759\n",
      "1759\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('output/titles.csv')\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('output/clean_titles.csv', index = False)\n",
    "## 1. download this as csv\n",
    "## 2. for excel version 16.89.1 , click the following :\n",
    "## 2.1 file \n",
    "## 2.2 import \n",
    "## 2.3 csv - select \"clean_titles.csv\"\n",
    "## 2.4 select 'delimited' and 'UTF-8' for file orign \n",
    "## 2.5 select 'comma' as delimiter then click finish. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f744cb1213c796c",
   "metadata": {},
   "source": [
    "### Word Clouds! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e04e462f3f0872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T00:49:37.061042Z",
     "start_time": "2024-08-28T00:49:37.043849Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/KenedyDucheine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "sw = set(stopwords.words('english'))\n",
    "sw.update(['harris', 'kamala', 'kamala harris', 'walz', 'tim walz', 'tim', 'say', 'lol', 'says', 'look', 'gave', 'see', 'new', 'likely', 'harriswalz', 'today', 'calls', 'still', 'really', 'reason', 'saying', 'happen', 'single'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380c81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13 weeks from inital data collection to election \n",
    "#df[\"date\"] = df[\"date\"].apply(lambda x: pd.to_datetime(x))\n",
    "# collection_date = df['date'].min()\n",
    "# print(collection_date)\n",
    "\n",
    "# # Calculate the week number (week 13, 13-1, 13-2, etc.)\n",
    "# df['week'] = 13 - ((df['date'] - collection_date).dt.days)//7\n",
    "\n",
    "# # Output the dataframe\n",
    "# print(df.head(14))\n",
    "# df.to_csv('output/dates_titles.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54690680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/dates_titles.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a797e1990a3d044d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T00:49:48.423654Z",
     "start_time": "2024-08-28T00:49:48.411985Z"
    }
   },
   "outputs": [],
   "source": [
    "def wc(dataframe):\n",
    "    df = dataframe\n",
    "    for i in df['subreddit_name_prefixed']: \n",
    "        \n",
    "        newdf = df[df['subreddit_name_prefixed']== i]\n",
    "        v = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        newdf = newdf.drop_duplicates(subset = ['title'])\n",
    "        newdf['title'] = newdf['title'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "        textlist = [word_tokenize(w) for w in newdf['title']]\n",
    "        tkwords = [word for n in textlist for word in n if word.lower() not in sw]\n",
    "        strings = ' '.join(tkwords)\n",
    "        #top = dict(Counter(strings).most_common(20))\n",
    "        \n",
    "        \n",
    "        wc = WordCloud(background_color= '#FFFFFF', colormap= 'cividis').generate(strings)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.axis('off')\n",
    "        plt.figtext(0.3,0.8,(f'{v} WordCloud: {date.today()}'),fontweight= 'bold')\n",
    "        plt.imshow(wc)\n",
    "        plt.savefig(f'output/wordclouds/{v}_wordcloud_{date.today()}.png')\n",
    "        plt.close()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e41073431e13e86b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T00:49:52.538181Z",
     "start_time": "2024-08-28T00:49:51.393913Z"
    }
   },
   "outputs": [],
   "source": [
    "wc(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (miniforge)",
   "language": "python",
   "name": "miniforge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
